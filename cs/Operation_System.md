## 프로세스 vs 쓰레드

1. 프로세스와 쓰레드 차이
    - 프로그램이란 특정 기능을 수행하기 위한 명령어들의 집합이며, 보통 exe 파일이나 아이콘으로 존재
    - 프로그램을 실행시키면 메인 메모리(RAM)에 올라가게 되고, 프로그램이 인스턴스화 된 상태를 프로세스라 함
    - 프로세스는 독립적인 주소 공간을 가지며, 각 주소 공간은 스택,힙,코드,데이터 세그먼트로 나뉨
    - 프로세스의 정보는 PCB에 저장되며, PCB는 커널 공간에서 관리함
    - 프로세스 간 통신은 제한적이며, IPC기법을 활용하면 OS에 의해 데이터 통신이 가능하게 도와줌
    - 쓰레드는 프로세스에서의 수행 단위를 의미하며, 하나의 프로세스는 여러개의 쓰레드를 가질 수 있음.
    - 쓰레드는 프로세스의 자원을 공유하며, 동일한 주소 공간 내에서 실행됨.

2. 프로세스 간 통신이 비효율적인 이유는 ?
    - 프로세스 간 통신을 위해서는 IPC 기법(shared memory / message queue / socket 등)을 활용해야 함
    - IPC 기법을 실행하는 도중, 데이터 복사 및 스위칭이 추가로 일어날 수 있음 => 오버헤드 발생
    - 쓰레드는 프로세스 메모리 중 스택 영역(함수,param,지역변수)을 제외한 모든 영역을 공유함
    - 별도의 IPC 기법 없이 통신이 가능함 (이미 메모리를 공유하기 때문에)
    - 따라서 쓰레드 간 통신 및 스위칭이 프로세스 간 통신 및 스위칭보다 오버헤드가 적음

3. IPC 기법 중 공유 메모리는 어떻게 프로세스 간 데이터 교환이 가능한지?
    - 공유 메모리 기법을 사용할 경우, 프로세스가 메모리에 올라올 시점에 데이터 영역 중 일부를 공유 메모리로 설정 (OS가)
    - 해당 영역에 대한 주소 및 접근 권한을 타 프로세스에게 부여
    - 해당 공유 영역에 데이터 엑세스(쓰기, 읽기)를 진행하며 데이터 교환이 가능함
    - 여러 프로세스가 동시에 데이터 엑세스를 진행할 경우 결과가 달라질 수 있으며, 이와 같은 영역을 임계 영역(Critical Section)이라 함
    - 임계 영역 문제를 해결하기 위해서 보통 공유 메모리 + 동기화 메커니즘 (MuTex, Semaphor, Monitor)를 같이 활용함

4. 프로세스와 쓰레드 중 어느 것이 자원 효율적인지?
    - Context Switching할 때를 생각해보면 쓰레드가 프로세스에 비해 더 효율적이다.
    - 컨텍스트 스위칭은 현재 실행 중인 프로세스/쓰레드의 상태를 저장하고, 새로운 프로세스/쓰레드와 교환하는 행위다.
    - 쓰레드의 경우 프로세스 내에서 실행되기 때문에 쓰레드의 스택 영역만 스위칭을 진행하면 된다.
    - 프로세스의 경우 현재 실행중이던 상태를 모두 저장하고, 나중에 다시 불러와서 작업을 재개하기 때문에 더 오래걸린다.
    - 프로세스 간 통신보다 쓰레드 간 통신이 오버헤드가 적게 발생하기 때문에 쓰레드가 더 효율적이다.

5. 컨텍스트 스위칭의 처리 과정 및 의의
    - 컨텍스트 스위칭은 현재 실행 중인 프로세스/쓰레드의 상태를 저장하고, 새로운 프로세스/쓰레드와 교환하는 행위다.
    - Interrupt 및 Syscall 호출: 
    - 컨텍스트 저장: 컨텍스트란 현재 실행중인 프로세스/쓰레드의 상태를 의미함 (CPU 레지스터 상태, PC, 우선순위 등)
    - 스케쥴러 활성화: 스케줄러가 활성화되어 다음에 실행시킬 프로세스를 결정
    - 컨텍스트 로드: 결정된 프로세스를 CPU에 로드함. PCB 정보를 활용하여 이전 작업중단 시점부터 실행 가능.
    장점
    - 여러 개의 프로세스를 메모리에 올려두고, 하나의 CPU에서 우선순위에 따라 실행시킴으로써 멀티 태스킹을 지원함
    - 우선순위에 따라 프로세스를 관리해줌으로써 효율적으로 자원을 사용할 수 있음.

6. 컨텍스트 스위칭 과정에서 발생할 수 있는 오버헤드
    - 보조기억장치에 데이터 엑세스를 진행할 경우 굉장히 느리기 때문에, 이러한 속도 차이를 방지하기 위해 캐시를 활용함. 
    - 컨텍스트 스위칭을 진행한다면 이전 프로세스에서 맞게 구성되어있던 캐시를 초기화해야하기 때문에 오버헤드 발생
    - PCB에 저장시킬 때/저장된 내용을 불러올 때 오버헤드가 발생
    - 가상 메모리 매핑 테이블을 현재 프로세스에 맞게 교체해야 함

7. 자식 프로세스가 상태를 알리지 않고 죽거나, 부모 프로세스가 먼저 죽을 경우 어떻게 처리할까요?
    - 자식 프로세스가 상태를 알리지 않고 죽는 경우 '좀비 프로세스'라고 부르며, 이러한 경우 부모 프로세스에서 주기적으로 자식에 대한 상태를 확인하며 해결할 수 있음
    - 부모 프로세스가 먼저 죽을 경우 자식 프로세스들을 '고아 프로세스'라 부르며, 리눅스 시스템에서의 최상단인 'init'에서 처리함으로써 해결할 수 있음.

## 시스템 콜 / 인터럽트

1. 시스템 콜이 무엇인가요? 작동 방식을 간단하게 말씀해주세요.
    - 시스템 콜은 소프트웨어 인터럽트라고도 불리며, 프로그램이 OS의 핵심 기능 및 서비스를 안전하게 요청할 수 있도록 도와주는 인터페이스임
    - OS에는 유저모드/커널모드 두 단계가 존재하며, 각각을 분리시킴으로써 시스템 내 자원을 보호할 수 있음
    - 유저 모드에서는 제한된 자원에만 접근이 가능하며, 커널 모드에서는 시스템 내 모든 자원에 접근할 수 있음
    - 사용자가 응용프로그램에서 I/O작업을 진행 -> 시스템 콜 호출(trap) -> 커널 모드에서 사용자 요청 처리 -> 결과 + 유저 모드로 복귀

2. 커널 모드에서 실행될 때 시스템 콜이 왜 필요한가요?
    - 사용자가 사용하는 응용프로그램에서 모든 자원에 접근할 수 있다면 악의적인 코드로 인해 자원 남용 및 시스템 장애 현상이 발생할 수 있음.
    - Syscall을 통해 안정성, 보안성 및 자원 효율성을 확보할 수 있음

3. 유저 모드와 커널 모드의 전환 과정은 어떻게 이뤄지나요?
    - 사용자의 응용 프로그램에서 Syscall이 호출되면, 현재 실행 중인 프로세스/쓰레드 상태를 저장함
    - 커널모드 전환 후 Syscall의 독자적인 PID와 함께 진행할 작업이 커널모드에서 작동하게 됨
    - 작업이 정상적으로 완료된 후, 유저 모드로 돌아오고 해당 작업에 대한 결과를 Syscall을 호출한 프로그램한테 전달

4. 시스템 콜과 인터럽트는 어떻게 다른가요?
    - Syscall은 사용자의 요청(파일 입출력, 네트워크 통신 등)에 의해 발생함.
    - Interrupt는 시스템 내/외부에서 발생하는 이벤트(입출력 장치, 예외 상황 등)에 의해 자동으로 발생함.

5. 인터럽트는 어떻게 처리되나요?
    - 하드웨어 장치나 예외 상황이 발생했을 경우 인터럽트 신호가 CPU에 전달
    - CPU는 인터럽트 신호를 감지하고 현재 실행 중인 프로세스 상태를 PCB에 저장
    - 인터럽트 신호와 매핑관 ISR(인터럽트 서비스 루틴)주소를 찾아 호출된 인터럽트 작업을 실행함.
    - 인터럽트 작업이 완료되었을경우, 원래 실행 중이었던 프로세스의 상태를 복원하고 작업을 재개함.

6. 하드웨어 인터럽트와 소프트웨어 인터럽트의 차이는?
    - 하드웨어 인터럽트는 외부 하드웨어 장치에 의해 발생하며, 키보드/마우스 등 실시간으로 발생하는 이벤트에 의해 자동 호출됨
    - 소프트웨어 인터럽트는 프로그램 코드에 의해 의도적으로 발생시키는 인터럽트로, Syscall이 대표적인 소프트웨어 인터럽트임

## CPU 스케쥴러
1. CPU 스케쥴링에서 고려해야할 주요 성능 지표는?
    - 처리량, 지연시간, 응답시간, CPU사용률 총 4가지를 고려해야 함
    - 처리량이란, 단위 시간당 어느정도의 작업을 처리할 수 있는지를 나타낸 지표
    - 지연시간이란 준비 큐(Ready Queue)에서 얼마만큼 기다렸는지를 나타내는 지표
    - 응답시간이란 준비 큐에 들어온 시점으로 부터 처음 응답했을때를 가리키는 지표
    - CPU 사용률이란 말 그대로 CPU의 어느정도를 사용하고 있는지를 나타내는 지표

2. 비선점/선점형 스케쥴링에 대해서 말씀해주세요
    - FCFS (First Come First Served): 가장 먼저 들어온 작업이 먼저 실행되는 방식
    - SJF (Shortest Job First): 준비 큐에 있는 작업 중 작업시간이 가장 작은 프로세스를 실행하는 방식
    - RR (Round Robin): 모든 프로세스가 일정 시간(CPU Time=Time Slice)만큼만 실행 후 다시 준비 큐로 돌아가는 방식
    - SRT (Shortest Remaining Time): 준비 큐에 있는 작업 중 남은 작업시간이 가장 작은 프로세스를 실행하는 방식
    - PQ (Priority Queue): 우선순위에 따라 프로세스를 실행시키는 방식
    - MLQ (Multi-Level Queue): 각각 다른 우선순위를 가지고 여러 개의 준비 큐 + 각 준비 큐는 고유 스케쥴링 방식을 가짐(FCFS / RR)

3. 우선순위 스케쥴링에서 발생할 수 있는 문제점은 어떤 것이 있을까요?
    - 기아 현상(starvation): 먼저 들어온 작업임에도 불구하고, 우선순위가 낮아 실행하지 못하는 현상 
    - 에이징 기법을 활용해서 해결 가능 (몇 차례 이상 실행 큐에 들어가지 못했을 경우, 다음 작업을 해당 작업으로 선정하는 방식)
 
4. 왜 RR은 공정한 스케쥴링 기법인가? 발생가능한 문제점은?
    - 비선점 스케쥴링의 경우 하나의 프로세스가 실행하기 시작하면, 종료될 때까지 기다려야 한다는 문제가 있음
    - 선점형 스케쥴링중 타 스케쥴링 기법은 '우선순위'에 의해 실행되지만, RR은 모든 작업이 Time Slice만큼 반복해서 실행되기 때문에 공정하다고 불림
    - RR에서 Time Slice를 어떻게 설정하는지에 따라 스케쥴링 성능에 영향을 미침.
    - Time Slice가 크면 하나의 작업이 오래 실행되기때문에 FCFS처럼 작동할 수 있음.
    - Time Slice가 작으면 Context Switching하는 비용이 많이 들기 때문에 CPU 사용률이 떨어지고, '쓰레싱' 현상이 발생할 수 있음

5. 동시성과 병렬성의 차이는 뭘까요?
    - 동시성과 병렬성의 가장 큰 차이점은 작업(프로세스 또는 쓰레드)의 실행 방식과 관련이 있음.
    - 동시성은 여러 작업이 마치 동시에 실행되는 것처럼 보이지만 실제로는 CPU가 빠르게 전환하면서 각 작업을 조금씩 번갈아 가며 실행하는 것을 의미함. 이는 단일 코어에서도 가능하며, 주로 멀티태스킹 또는 멀티프로그래밍 환경을 제공.
    - 병렬성은 실제로 여러 작업이 동시에 실행되는 것을 의미하며, 이는 여러 개의 CPU 코어 또는 여러 프로세서를 활용하여 여러 작업을 물리적으로 동시에 수행함. 주로 멀티쓰레딩이나 멀티프로세싱과 연관이 있음.

6. 단기/중기/장기 스케쥴링 알고리즘에 대해서 설명해주세요
    - 단기 스케줄러는 준비 큐(Ready Queue)에서 어떤 작업(프로세스)에게 CPU를 할당할지를 결정함.
    - 중기 스케쥴링은 시스템의 메모리 부하를 조절하기 위해 사용되며, 메모리가 부족할 경우 Swap In/Out을 진행하는 역할을 함
    - 장기 스케쥴링은 디스크에 존재하는 작업 중 어떤 작업을 준비 큐(Ready Queue)에 넣어줄지 결정하는 역할을 함

7. 프로세스 스케줄링 상태
    - Ready: 프로세스가 실행을 위해 준비가 된 상태로, CPU를 할당받기위해 기다리고 있는 상태
    - Running: 하나의 프로세스가 CPU를 할당받아 실행 중인 상태
    - Waiting: Running 상태의 프로세스가 작업을 진행하다 I/O와 같은 이벤트가 실행될 때 완료되기 까지를 기다리는 상태
    - Terminated: 프로세스가 작업을 모두 마치고 종료된 상태

## 동시성 제어 매커니즘

1. 동시성 제어 메커니즘이란 무엇인가요?
    - 동시성 제어 메커니즘이란 멀티태스킹 환경에서 여러 프로세스나 쓰레드가 데이터를 동시에 엑세스할 경우 발생할 수 있는 충돌 및 일관성 문제를 방지하기 위해 사용됨
    - 락, 뮤텍스, 세마포어, 모니터 등이 존재함

2. 락과 뮤텍스의 차이점은?
    - 락과 뮤텍스는 둘 다 락킹 메커니즘의 일부라는 공통점을 가지고 있음.
    - 락킹 메커니즘이란 Lock()과 Unlock() 함수를 실행하며 특정 자원을 한 번에 하나의 프로세스만 사용할 수 있도록 제어하는 것.
    - 일반적인 락은 보통 어떤 프로세스나 쓰레드가 Unlock()을 호출하여 자원을 해제할 수 있음.
    - 뮤텍스는 Lock()을 호출한 동일한 프로세스나 쓰레드만이 Unlock()을 호출할 수 있음.

3. 뮤텍스 vs 세마포어 
    - 뮤텍스는 임계 영역에 대해 하나의 프로세스만 접근하여 사용할 수 있도록하는 락킹(Locking) 메커니즘 중 하나임.
    - A라는 프로세스가 임계 영역에 접근했을 경우 잠금(Lock)을 진행하면 타 프로세스(B, C, ..)는 임계 영역에 접근하지 못함.
    - A가 작업을 모두 완료했을 때 해제(Unlock)을 진행하고, 또 다른 프로세스가 해당 임계 영역에 접근할 수 있게 함
    - Python에서 Multi-Thread가 제한되는 이유는 GIL(Global Interpreter Lock) 때문이며, 이는 뮤텍스와 유사하게 작동하여 한 번에 하나의 쓰레드만이 Python 바이트코드를 실행하도록 하여 경쟁 조건(Race Condition)을 없애줌.
    - 세마포어는 최대 접근 자원 수를 제한하는 매커니즘임
    - 예를 들어, 화장실/프린터를 생각해보면, 사용자는 10명이고 자원은 3개일 경우 3개 이상으로 사용자를 받을 수 없게 됨.
    - P함수 (자원 할당)와 V함수 (자원 해제)를 활용하여 관리 

4. 모니터란?
    - 모니터 큐(Monitor Queue)를 자원과 프로세스 사이의 인터페이스로 작동함
    - 특정 자원에 프로세스가 직접 접근하는 것을 막고 모니터 큐에 넣은 후 순차적으로 실행하면서 자원 할당 문제를 해결하는 방식
    - 복잡한 조건에 따른 동기화가 필요할 때 효과적으로 사용될 수 있음

5. 데드락과 경쟁조건은 동시성 제어와 어떤 관련이 있는지?
    - 데드락은 두 개 이상의 프로세스가 서로의 작업 완료를 무한히 기다리는 상태를 의미
    - '무한정 대기', '비선점', '원형 대기', '점유 후 대기' 총 4가지를 모두 만족해야 교착상태로 정의할 수 있음
    - 경쟁 조건은 두 개 이상의 프로세스가 동시에 같은 데이터를 수정하려고 할 때, 프로세스의 순서에 따라 데이터의 최종 상태가 변경되는 경우를 의미
    - 뮤텍스나 락은 경쟁 조건(일관성 문제)를 방지하고, 세마포어나 모니터는 데드락(교착 상태) 가능성을 줄이는데 도움을 줌

6. 데드락을 관리할 수 있는 방법 4가지
    - 예방: 
        - 데드락이 발생할 수 있는 조건 중 한 가지 이상을 차단하여 예방하는 방법
        - 예방 기법을 활용할 경우 자원의 사용을 제한하기 때문에 효율적인 자원 관리를 진행하지 못할 수 있음
    - 회피
        - 시스템이 자원을 할당할 때 데드락이 발생하지 않도록 사전에 회피하는 방법 (은행원 알고리즘)
        - 모든 프로세스가 요구할 자원의 최대 수량을 미리 알아야 하며, 자원 상태를 지속적으로 추적해야 함(비용)
    - 탐지
        - 데드락이 발생했는지 주기적으로 검사하며 탐지하는 방법
        - 탐지 후 회복하는 절차가 필요함. 탐지 + 회복 => 오버헤드 발생
    - 회복
        - 데드락을 탐지한 후에 해결하는 방법으로, 주로 프로세스 중단 or 자원을 강제로 회수함
        - 프로세스 중단이나 강제적 자원 회수는 시스템의 불안정성을 초래할 수 있음

7. 현대 OS에서는 왜 데드락을 따로 처리하지 않을까요?
    - 예방,회피,탐지,회복 기법 모두 상당한 오버헤드가 발생하며, 복잡한 자원관리 기법을 구현하고 유지보수하는 것이 매우 제한적임
    - 데드락이 발생할 case가 많지 않은데 복잡하게 모든 case를 분석하여 처리하는 것이 효율적이지 못함

## 메모리 세그먼트

1. 메모리 세그먼트란?
    - 메모리의 관리 및 효율적인 사용을 위해 여러 개의 세그먼트(코드/데이터/스택/힙)으로 분할됨
    - 코드: 프로그램의 실행파일에서 기계어 코드가 저장되는 영역
    - 데이터: 전역/정적 변수가 존재하는 영역
    - 스택: 함수 호출, parameter, 지역 변수 등을 저장하는 영역
    - 힙: 동적 메모리 할당(malloc, arrayList, new, ...)을 위해 사용되는 영역

2. 코드 세그먼트가 분리되어 있는 이유?
    - 코드 세그먼트에는 기본적으로 프로그래머가 작성한 코드가 기계어로 저장되어있음
    - 해당 영역을 마음대로 조절이 가능하다면 원래 실행하려고 했던 순서대로 작동하지 않을 우려가 발생함
    - 코드 영역을 따로 관리하고 해당 영역에 대한 접근을 제한함으로써 보안 효과를 얻을 수 있음

3. 데이터 세그먼트에서 '정적 변수'와 '전역 변수'의 차이는?
    - 정적 변수와 전역 변수는 프로그램이 종료되기 이전까지 값이 유지된다는 특징을 가지고 있음
    - 정적(static) 변수는 함수나 클래스 안에서 static 키워드를 사용하여 선언된 변수를 의미하며, 해당 함수나 클래스를 호출한 후에도 내부에서 값이 유지되는 변수를 의미함
    - 전역(global) 변수는 프로그램 내부 모든 곳에서 접근 가능한 변수를 의미함

4. 메모리 관리를 잘못했을 경우 발생하는 에러 두 가지
    - Stack Overflow: 프로세스가 스택 세그먼트의 최대 할당량을 초과하는 메모리를 필요로 할 때 발생 (무한정 재귀)
    - Segmentation Fault: 프로세스가 허용되지 않은 메모리 영역에 접근할 때 발생 (잘못된 포인터 참조)


## 컴파일 과정

1. 컴파일이 어떻게 이뤄지는지 말해주세요
    - 컴파일은 전처리 -> 컴파일 -> 어셈블 -> 링킹 총 4단계로 작동됨
    - 전처리: 소스코드 내 전처리 지시자(#include 등)를 처리함
    - 컴파일: 소스코드를 어셈블리어로 변경하는 작업을 진행
    - 어셈블: 어셈블리어를 기계어로 변환하는 작업을 진행 (Object file)
    - 링킹: 여러 Object file + Library를 결합하여 실행가능한 파일을 생성하는 단계 (exe file)

2. 컴파일 언어 vs 인터프리터 언어
    - 컴파일 언어
        - 컴파일을 거친 후 결과물인 실행 파일을 활용하여 프로그램 실행
        - 실행 속도가 빠르며, C/C++ 등이 대표적인 컴파일 언어
    - 인터프리터 언어
        - 실행할 때(Runtime) 코드를 해석하고 실행하는 언어를 의미
        - 실행 속도가 상대적으로 느리며, JIT(Just In Time) 컴파일러를 활용하여 성능 개선 가능
        - Python, JavaScript 등이 대표적인 인터프리터 언어

## 캐시 메모리

1. 캐시란 무엇인가?
    - 캐시는 메모리 계층 구조의 일부로, 주로 CPU와 주 메모리 사이에 위치하여 시스템의 성능과 효율성을 향상시키는 메커니즘임
    - 보조기억장치(HDD/SSD)는 주 메모리(RAM)나 캐시에 비해 데이터 접근 속도가 느리며, 오버헤드가 큼
    - 캐시는 주로 CPU와 주 메모리 사이의 속도 차이를 줄이기 위해 사용되며, 자주 사용되는 데이터나 명령어를 캐시에 저장하여 데이터 접근 시간을 줄임

2. 캐시의 종류에 대해서 설명해주세요.
    - 캐시는 L1, L2, L3캐시로 나뉘게 되며, CPU에 가까울수록 메모리 크기는 작지만 비쌈
    - L1 캐시는 보통 각 CPU 코어에 독립적으로 존재하며, 데이터 캐시와 명령어 캐시로 분리된 분리 캐시 구조를 가짐
    - L2 캐시는 L1 캐시보다 크고, 각 CPU 코어에 독립적으로 존재하거나 몇몇 코어들이 공유할 수 있음
    - L3 캐시는 모든 CPU 코어가 공유하는 캐시로, L2 캐시보다 더 큼

3. 캐시의 작동 원리를 설명해주세요
    - 캐시는 지역성의 원리를 이용함. 프로그램이 실행되는 동안 데이터와 명령어 접근 패턴이 시간/공간적으로 가까운 경향이 있다는 것을 의미
    - 시간적 지역성은 최근에 참조한 데이터를 다시 접근 할 가능성이 높음을 의미하며, 공간적 지역성은 참조한 데이터와 인접한 데이터를 곧 접근할 가능성이 높음을 의미함
    - 캐시는 태그로 관리되며, CPU가 필요한 데이터를 캐시에서 찾으면 캐시 히트(Hit), 찾지 못하면 캐시 미스(Miss)라고 함. (캐시 히트 수/전체 접근 수 => 히트율(Hit Ratio)이라고 부름.)
    - 캐시가 꽉 찼을 경우, 적절한 캐시 교체 정책(LRU, LFU, FIFO 등)에 의해 기존 데이터를 교체하여 관리함

4. 캐시 매핑에 대해서 설명하세요
    - 캐시 메모리에 데이터를 저장하는 방법은 대표적으로 3가지가 존재함
    - 직접 사상(DM): 
        - 각 메모리 블록은 캐시의 특정 슬롯에만 저장되는 특징을 가짐. 
        - 메모리 50개 캐시 5개면 1-10 -> 캐시 1번 이런식으로 관리함
        - 구현이 간단하고 빠르지만 캐시 충돌이 자주 일어나 비효율적일 수 있음
    - 연관 사상(AM): 
        - 메모리 블록은 어느 슬롯에도 저장될 수 있으며, 모든 캐시라인을 검사하여 데이터를 찾거나 저장함
        - 메모리 50개 캐시 10개면 전체 탐색하면서 비어있는 슬롯에 저장하며 관리
        - 구현이 복잡하고 탐색 속도가 길어질 수 있음
    - 집합 연관 사상(S-AM): 
        - 캐시는 여러 개의 집합으로 나뉘며, 각 집합 내에는 연관 사상이 적용됨
        - 메모리 50개 캐시 10개면 1-25 -> 캐시 1~5번 블럭에 매핑하는 방식으로 관리함
        - 구현이 복잡하지만, 직접/연관 사상의 장점을 결합하여 만들었기 때문에 성능은 우수함

5. 캐시 간 동기화는 어떻게 이뤄지나요?
    - 캐시 일관성은 여러 캐시가 있는 시스템에서 동일한 메모리 위치에 대한 모든 캐시의 값이 동일하게 유지되는 것을 의미
    - MESI 프로토콜을 활용하여 캐시 일관성을 유지할 수 있음

6. 캐시 교체 정책에 대해서 아는대로 설명해보세요
    - 캐시가 가득 찬 경우, 새 데이터를 캐시에 저장하기 위해 기존 데이터를 교체해야 하며, 이때 사용하는 것이 캐시 교체 정책임
    - FIFO(First In First Out)은 가장 단순하게 구현가능하며, 들어온 순서대로 데이터를 교체하는 방식임
    - LRU(Least Recently Used)는 가장 오랫동안 사용하지 않은 데이터를 교체하는 방식임
    - LFU(Least Frequently Used)는 사용 빈도가 가장 적은 데이터를 교체하는 방식임
    - LRU는 언제 마지막으로 호출되었는지, LFU는 몇 번 호출되었는지에 대한 상태를 저장해야 하기에 구현이 복잡할 수 있음

7. Write-Through와 Write-Back에 대해서 설명해보세요
    - 두 가지 기법은 대표적인 캐시의 쓰기 정책임. 언제 캐시를 업데이트(쓰기)하는지에 따라 두 가지로 분류됨
    - Write-through의 경우 데이터를 캐시에 쓰는 동시에 주 메모리에도 쓰는 방식으로, 데이터 일관성을 유지하기는 좋지만 쓰기 성능이 떨어질 수 있음(자주 쓰니까 오버헤드 발생)
    - Write-back의 경우 데이터를 캐시에만 쓰고 캐시 라인이 교체될 때 주 메모리에 쓰는 방식으로, 쓰기 성능이 향상되지만 데이터의 일관성이 항상 보장되지는 않음

## Sync & Async // Blocking & Non-Blocking

1. Sync & Async // Blocking & Non-Blocking 개념을 설명해주세요.
    - 동기(Sync): 요청한 작업이 종료될 때까지 대기하는 방식을 의미하며, 요청한 순서대로 작업이 완료됨
    - 비동기(Async): 작업을 요청한 후에 종료될 때까지 대기하지 않고 다른 작업을 수행하는 방식을 의미하며, 요청 순서와 상관없이 작업이 완료되면 callback 함수를 통해 결과를 반환 받음
    - 블로킹(Block): 하나의 작업이 수행되어 완료되기 전까지 호출자를 대기시키는 방식
    - 논블로킹(NonBlock): 작업 수행과 동시에 결과를 바로 반환하거나 작업을 

2. 동기와 비동기의 차이를 예시로 설명해보세요.
    - 클라이언트가 웹 서버에 데이터를 요청하면, 서버는 DB에서 조회와 같은 작업을 수행한다고 가정
    - 동기적으로 수행하면 DB에서 쿼리가 완료되고 응답이 올 때까지 다른 작업을 수행하지 못함 (대규모 트래픽 발생 시 성능 저하)
    - 비동기적으로 수행하면 DB에서 쿼리작업의 응답이 오기를 기다리며 다른 작업을 동시에 처리할 수 있음
    - 동기와 비동기의 차이는 작업 순서 처리의 차이임 (순서 보장 / 순서 미보장)

3. 블로킹과 논블로킹의 차이를 예시로 설명해보세요.
    - 시스템 내에서 파일을 읽는 작업을 진행한다고 가정
    - 블로킹의 경우 파일을 다 읽을 때까지 다른 작업을 진행하지 않은 채 대기
    - 논블로킹의 경우 파일을 다 읽지 않아도 다른 작업을 수행할 수 있음
    - 블로킹과 논블로킹은 전체적인 작업의 흐름 자체를 막는지/안막는지(제어권을 주는지 안주는지)의 차이임 (직렬 실행 / 병렬 실행)

## 가상(논리)메모리

1. 가상 메모리란?
    - 가상 메모리란 메모리 관리 기법 중 하나로, 실제 물리 메모리보다 큰 메모리를 프로그램이 사용할 수 있도록 해줌
    - 주 메모리(RAM) + 보조기억장치 일부(HDD/SSD)를 메모리로 활용함으로써 크고 복잡한 어플리케이션 또한 작동이 가능함
    - 가상 메모리는 페이지라는 작은 블록들로 나누어 관리하며, 필요에 따라 페이지를 스왑 영역에 IN/OUT하며 메모리 관리를 진행함.

2. 가상 주소와 물리 주소의 차이점은?
    - 가상 주소는 실제로 프로그램이 사용하는 주소를 의미하며, 프로그램 코드에 의해 참조됨
    - 물리 주소는 실제 메모리(RAM)의 위치를 가리키는 주소를 의미함
    - 페이지 테이블이란 가상 주소와 물리 주소를 매핑하는 데이터 구조로, 가상 주소를 입력받아 해당 가상주소에 대응하는 물리 주소를 반환함

3. 페이지 테이블은 가상 주소와 물리 주소를 어떻게 매핑하나요?
    - 페이지 테이블은 가상 주소와 물리 주소를 연결하는 중요한 데이터 구조임
    - 가상 주소는 페이지 번호 + 오프셋으로 나눌 수 있음
    - 페이지 번호가 페이지 테이블에서 index로 사용되어 테이블 엔트리를 찾음. 해당 엔트리에는 실제 주 메모리에서의 시작 주소를 포함
    - 엔트리 내 실제 주소 시작 번호 + 오프셋을 통해 실제 물리 주소를 결정함
    - 페이지 테이블은 TLB(Translation Lookaside Buffer)라는 캐시 메모리를 사용하여 자주 참조되는 페이지 매핑 정보를 빠르게 제공함

4. 스왑영역이 무엇이고, 스와핑이 어떤 행위인지 설명해보세요.
    - 스왑영역이란 보조기억장치의 일부를 사용하여 현재 사용하지 않는 메모리 페이지를 임시로 저장하는 영역
    - 물리 메모리가 부족할 때, 필요없는 page를 swap out함으로써 필요한 메모리를 확보하고, 추후에 swap out된 page가 필요할 경우 다시 swap in 함으로써 사용하며, 이와 같은 방식을 스와핑(swapping)이라고 정의함
    - OS에 의해 자동으로 일어나는 스와핑에 인해 효율적으로 메모리를 사용할 수 있게 됨
    - 스와핑은 보조기억장치에 I/O를 진행하기 때문에 오버헤드가 상당하므로, 너무 많은 스와핑은 성능 저하의 원인으로 이어짐

5. Page Fault가 무엇인가요?
    - page fault란 페이지 테이블을 통해 가상 주소를 물리 주소로 변환했는데, 실제 메모리에 존재하지 않는 경우를 의미함
    - page fault가 발생할 경우 os에 의해 보조기억장치 내 필요한 데이터를 실제 메모리로 로드하는 작업을 진행함
    - 실제 메모리가 다 찼을 경우, 페이지 교체 정책에 의해 교체하여 데이터를 저장함

6. 페이지 교체 알고리즘이란 무엇이고, 어떤 것들이 있는지 말해보세요.
    - 스왑 영역내에서 어떤 페이지를 메모리로 가져오고, 어떤 페이지를 스왑 영역에 임시 저장할 지 선택하는 알고리즘을 의미
    - FIFO(First In First Out): 가장 먼저 메모리에 올라온 페이지를 가장 먼저 swap out함
    - LRU(Least Recently Used): 가장 오랫동안 사용하지 않은 페이지를 swap out함. (추가적인 작업을 통해 언제 실행되었는지에 대한 정보를 저장해야 함)
    - Optimal: 미래에 가장 오랫동안 사용하지 않을 페이지를 교체함. 실질적으로 어떤 프로세스가 언제 실행 될지를 모르기 때문에 제한적임

7. TLB란?
    - TLB란 CPU가 메모리에 접근할 때, 가상 주소를 물리 주소로 매핑하는 과정에서 데이터를 엑세스하는 시간을 줄이기 위한 캐시 메모리의 일부임
    - 가상 주소를 물리 주소로 변환할 때, MMU에 의하여 TLB 내에 먼저 가상 주소가 존재하는지 여부를 판단 (존재할 경우 hit, 존재하지 않을 경우 miss로 판단)
    - miss의 경우 page table을 다시 검색하여 물리 주소를 찾고, 이를 활용하여 TLB 최신화 진행
    - 물리 주소를 못찾을 경우 Page Fault 현상이라 정의하며, os에 의해 보조기억장치에 접근하여 필요한 데이터 로딩
